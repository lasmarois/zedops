# Task Plan: Port Validation & Availability Checking

**Goal:** Add comprehensive port validation to prevent server creation failures due to port conflicts

**Status:** ‚úÖ Complete (All Phases Complete: 1, 2, 2b, 3, 4)
**Priority:** High (blocks Milestone 4 Phase 5 completion)
**Created:** 2026-01-10
**Last Updated:** 2026-01-10
**Completed:** 2026-01-10

---

## ‚ö†Ô∏è CRITICAL ARCHITECTURE CHANGE (2026-01-10)

**Discovery:** Port checking via `ss` command doesn't work in containerized agent due to network namespace isolation.

**Root Cause:**
- Agent runs in Docker container with its own network namespace
- `ss -tuln` inside container shows container's network stack, not host's
- Result: Always finds "0 bound ports at host level" even though ports ARE bound

**Decision:** **Switch to host-based agent deployment** (Milestone 5)
- Agent will run natively on host as systemd service (not containerized)
- Docker used ONLY to build the binary (cross-compile support)
- Binary runs directly on host with full network access

**Impact on Port Validation:**
- ‚úÖ Port checking will work correctly (no namespace isolation)
- ‚úÖ Can use `ss` command or read `/proc/net` directly
- ‚úÖ Enables host metrics collection (CPU, RAM, disk)
- ‚úÖ Matches production architecture specification

**See:** MILESTONES.md - Milestone 5: Agent Installation & System Service

---

## Problem Statement

Current issues discovered during testing:
1. ‚ùå Server creation fails with "port already allocated" error
2. ‚ùå No pre-flight port availability checking
3. ‚ùå Failed servers remain in DB with status='failed'
4. ‚ùå No way to edit/retry failed server configs
5. ‚ùå UI doesn't allow port customization
6. ‚ùå Auto-suggest ports without checking actual availability

**Example Error:**
```
failed to start container: Bind for :::16261 failed: port is already allocated
```

**Current Port Usage (Host Level):**
- 16261-16262: steam-zomboid-newyear (UDP)
- 16263-16264: steam-zomboid-build42-jan26 (UDP)
- 26261-26262: steam-zomboid-servertest (UDP)

---

## Architecture Design

### Port Checking Layers

**Layer 1: Database Check (Manager)**
- Query `servers` table for existing port allocations
- Fast, synchronous check before agent communication

**Layer 2: Docker Container Check (Agent)**
- Query Docker API for all container port bindings
- Includes stopped containers (may restart)

**Layer 3: Host Network Check (Agent)**
- NEW: Use `ss` or `/proc/net` to check actual bound ports
- Catches non-Docker services using ports

### New Message Protocol

**Request:** `port.check`
```json
{
  "subject": "port.check",
  "data": {
    "ports": [16265, 16266, 27017]
  },
  "reply": "_INBOX.uuid"
}
```

**Response:**
```json
{
  "subject": "_INBOX.uuid",
  "data": {
    "available": [16265, 16266],
    "unavailable": [
      {
        "port": 27017,
        "reason": "bound by process 'mongod' (PID 1234)"
      }
    ]
  }
}
```

---

## Implementation Phases

### Phase 1: Agent Port Checking (Backend)
**Status:** ‚úÖ Complete

**Tasks:**
- [x] Add `port.check` message handler to agent
- [x] Implement `CheckPortAvailability()` function
  - Check Docker containers (all containers including stopped)
  - Check host network bindings (`ss -tuln`)
  - Return detailed availability status
- [ ] Add unit tests for port checking logic (deferred to Phase 5)
- [x] Update agent message routing

**Files Created/Modified:**
- `agent/network.go` (NEW) - Port availability checking logic
  - `CheckPortAvailability()` - Main function checking Docker + host
  - `getHostBoundPorts()` - Parses `ss -tuln` output
  - `extractPortFromAddress()` - Helper for parsing network addresses
  - `SuggestNextAvailablePorts()` - Auto-suggest algorithm
  - `PortCheckRequest`, `PortAvailability`, `PortConflict` types
- `agent/main.go` - Added `handlePortCheck()` handler and message routing
- `agent/Dockerfile.build` - Rebuilt agent image
- `CLAUDE.md` - Added agent rebuild workflow documentation

**Implementation Notes:**
- Uses 3-layer checking: DB (future), Docker containers, host network
- Graceful degradation if `ss` command fails (logs warning, continues with Docker-only check)
- Returns detailed conflict info (port, reason, source: "docker" or "host")
- Agent successfully deployed and connected to manager

**Estimated Complexity:** Medium (actual: Medium)

---

### Phase 2: Manager Port Validation API (Backend)
**Status:** ‚úÖ Complete

**Tasks:**
- [x] Add GET `/api/agents/:id/ports/availability` endpoint
  - Query servers table for DB-level conflicts
  - Forward to agent for host-level check
  - Combine results
  - Suggest next available port range
- [x] Update AgentConnection DO to handle port.check forwarding (‚úÖ completed in Phase 1)
- [x] Add port conflict detection to server creation flow

**Files Modified:**
- `manager/src/routes/agents.ts`:
  - Added GET `/api/agents/:id/ports/availability` endpoint (lines 235-355)
  - Updated POST `/api/agents/:id/servers` with 2-layer validation (lines 657-694):
    1. Database conflict check (game_port, udp_port, rcon_port)
    2. Host-level availability check via agent
- `manager/src/durable-objects/AgentConnection.ts` - ‚úÖ Already forwarding port.check (Phase 1)

**Implementation Details:**

**1. Port Availability Endpoint:**
```typescript
GET /api/agents/:id/ports/availability?count=3

Response:
{
  suggestedPorts: [
    { gamePort: 16265, udpPort: 16266, rconPort: 27017 },
    { gamePort: 16267, udpPort: 16268, rconPort: 27018 },
    { gamePort: 16269, udpPort: 16270, rconPort: 27019 }
  ],
  allocatedPorts: [
    { gamePort: 16261, udpPort: 16262, rconPort: 27015, serverName: "bingchilling", status: "failed" },
    { gamePort: 16263, udpPort: 16264, rconPort: 27016, serverName: "bonjour", status: "failed" }
  ],
  hostBoundPorts: [16261, 16262, 16263, 16264],
  agentStatus: "online"
}
```

**Logic:**
- Queries DB for all allocated ports
- Checks host-level port availability (16261-16400, 27015-27100)
- Combines DB + host bindings to find next N available port ranges
- Auto-increments: game +2, rcon +1

**2. Server Creation Validation:**
```typescript
POST /api/agents/:id/servers
{
  name: "testserver",
  imageTag: "latest",
  config: {...},
  gamePort: 80,    // Optional - will auto-suggest if not provided
  udpPort: 81,     // Optional
  rconPort: 443    // Optional
}

Error response (409):
{
  error: "Port conflict detected on host: 80 (Used by container 'nginx'), 443 (Used by container 'nginx')",
  suggestion: "Use GET /api/agents/:id/ports/availability to find available ports"
}
```

**Validation Flow:**
1. Auto-suggest ports if not provided (MAX(game_port) + 2, MAX(rcon_port) + 1)
2. Check DB for conflicts (SELECT ... WHERE game_port = ? OR udp_port = ? OR rcon_port = ?)
3. Query agent for host-level conflicts (POST /ports/check)
4. Return 409 with detailed error message if any conflict found
5. Create server if all ports available

**Testing Results:**

Test 1: DB conflict (port 16261 already in DB)
```bash
curl -X POST ... -d '{"name":"testserver", "gamePort":16261, ...}'
# Response: "Port conflict in database: One or more ports (16261, 16262, 27015) already allocated to server 'bingchilling'"
```
‚úÖ PASS: DB-level validation working

Test 2: Host conflict (nginx using port 80, 443)
```bash
curl -X POST ... -d '{"name":"testserver2", "gamePort":80, "rconPort":443, ...}'
# Response: "Port conflict detected on host: 80 (Used by container 'nginx'), 443 (Used by container 'nginx')"
```
‚úÖ PASS: Host-level validation working with detailed conflict info

Test 3: Available ports (16267-16268, 27018)
```bash
curl -X POST ... -d '{"name":"testserver", "gamePort":16267, ...}'
# Response: {"success": true, "server": {..., "status": "running"}}
```
‚úÖ PASS: Server created successfully with available ports

**Complexity:** Medium (actual: Medium)

**Completed:** 2026-01-10

---

### Phase 2b: Server Rebuild (Image Update)
**Status:** ‚úÖ Complete

**Background:**
During Phase 2 testing, discovered that existing servers don't update their image when registry `:latest` tag changes. Current `restart` only restarts the same container with old image. Need explicit rebuild functionality.

**Tasks:**
- [x] Add `RebuildServer()` function in agent (`agent/server.go`)
  - Get existing server config from container inspection
  - Pull latest image from registry
  - Stop and remove old container
  - Create new container with same volumes, ports, config
  - Return new container_id
- [x] Add `server.rebuild` message handler in agent (`agent/main.go`)
- [x] Add `POST /api/agents/:id/servers/:serverId/rebuild` endpoint (manager)
- [x] Add `handleServerRebuild()` in AgentConnection DO
- [x] Update servers table with new container_id after rebuild
- [x] Test rebuild preserves volumes and game data

**Implementation Plan:**

**Agent Side:**
```go
// agent/server.go
func (dc *DockerClient) RebuildServer(ctx context.Context, containerID string) (string, error) {
  // 1. Inspect existing container to get config
  // 2. Pull latest image (digest check)
  // 3. Stop & remove old container
  // 4. Create new container with same volumes, ports, env
  // 5. Start new container
  // 6. Return new containerID
}
```

**Manager Side:**
```typescript
POST /api/agents/:id/servers/:serverId/rebuild
// 1. Get server from DB (container_id, image_tag)
// 2. Forward to AgentConnection DO -> agent
// 3. Update DB with new container_id
// 4. Return updated server
```

**Benefits:**
- ‚úÖ Updates server to latest registry image
- ‚úÖ Preserves volumes (game saves intact)
- ‚úÖ Preserves ports and configuration
- ‚úÖ Explicit user control (no automatic surprises)
- ‚úÖ Works with any tag (`:latest`, `:2.1.0`, etc.)

**Use Cases:**
- Update server to latest game version
- Apply registry image updates (bug fixes, features)
- Refresh server without losing game data

**Files Modified:**
- `agent/server.go` - Added `RebuildServer()` function (line 216-330)
- `agent/main.go` - Added `server.rebuild` handler (line 279-763)
- `manager/src/routes/agents.ts` - Added rebuild endpoint (line 979-1079)
- `manager/src/durable-objects/AgentConnection.ts` - Added DO handler (line 109-1309)

**Testing Results:**

**Test 1: Rebuild existing server**
```bash
# Create test server
curl -X POST .../servers -d '{"name":"rebuild-test-v2", ...}'
# Response: container_id: 858dd05d8284d3d462facd731fed66d0433407c7ff9aed91bd6163fc33a1cbda

# Rebuild server
curl -X POST .../servers/0d2a8ec1-8452-41e5-b657-14ab2c78e1aa/rebuild
# Response: container_id: d9b4696f93d1b39b818b5617f07e82fbda1c3040a35aed2d6d08c17e3c9e0562
```
‚úÖ PASS: Container ID changed (old container removed, new container created)

**Test 2: Verify rebuild process (agent logs)**
```
Rebuilding server container: 858dd05d8284...
Container config extracted: image=...latest, name=steam-zomboid-rebuild-test-v2
Pulling latest image: registry.gitlab.nicomarois.com/nicolas/steam-zomboid:latest
Image pulled: ...latest
Stopping old container: 858dd05d8284...
Removing old container: 858dd05d8284...
Creating new container: steam-zomboid-rebuild-test-v2
New container created: ...d9b4696f93d1...
New container started successfully
Server rebuild complete: 858dd05d8284 -> d9b4696f93d1
```
‚úÖ PASS: Complete rebuild workflow executed (pull ‚Üí stop ‚Üí remove ‚Üí create ‚Üí start)

**Test 3: Verify volumes preserved**
```bash
docker inspect d9b4696f93d1 --format '{{json .Mounts}}'
# Shows: /var/lib/zedops/servers/rebuild-test-v2/bin
#        /var/lib/zedops/servers/rebuild-test-v2/data
```
‚úÖ PASS: Same volume paths preserved (game data intact)

**Test 4: Verify container running**
```bash
docker ps --filter "name=steam-zomboid-rebuild-test-v2"
# Up 30 seconds (health: starting)
```
‚úÖ PASS: New container started successfully with same name

**Implementation Notes:**
- Rebuild timeout set to 60 seconds (includes image pull time)
- Database status transitions: running ‚Üí rebuilding ‚Üí running (or failed)
- Preserves: volumes, ports, networks, environment variables, labels
- Updates: container_id, image digest (if registry updated)

**UI Implementation:**

**Files Modified:**
- `frontend/src/lib/api.ts` - Added `rebuildServer()` API function
- `frontend/src/hooks/useServers.ts` - Added `useRebuildServer()` hook
- `frontend/src/components/ContainerList.tsx` - Added rebuild button and handler

**UI Features:**
- Blue "Rebuild" button next to "Delete Server" button
- Confirmation dialog: "Rebuild server 'X'? This will pull the latest image and recreate the container. Game data will be preserved."
- Loading state: "Rebuilding..." during operation
- Success/Error toast notifications
- Auto-refresh after rebuild completes
- Only shown for ZedOps-managed servers

**Test 5: UI Button Works**
```
‚úÖ PASS: Button appears for ZedOps-managed servers
‚úÖ PASS: Confirmation dialog shows before rebuild
‚úÖ PASS: Loading state displays during rebuild
‚úÖ PASS: Success message shown after completion
‚úÖ PASS: Container list refreshes automatically
```

**Complexity:** Medium (actual: Medium)

**Completed:** 2026-01-10 (Backend + Frontend UI)

---

### Phase 3: Enhanced UI Port Selection (Frontend)
**Status:** ‚úÖ Complete

**Tasks:**
- [x] Update ServerForm component
  - Add collapsible "Port Configuration" section
  - Show auto-suggested ports (with availability check)
  - Add "Check Availability" button
  - Show port status indicators (‚úì available / ‚úó in use)
  - Allow manual port override
- [x] Add port availability API hook
  - `usePortAvailability(agentId, ports)` hook
  - Real-time availability checking
- [x] Improve error display
  - Show specific port conflict errors
  - Suggest alternative ports
  - Agent offline error handling

**Files Modified:**
- `frontend/src/components/ServerForm.tsx` - Enhanced port UI (273 lines added)
  - Added collapsible "Port Configuration" section
  - "Check Port Availability" button (teal/info color)
  - Real-time port status indicators (‚úì available / ‚úó in use / ? unknown)
  - Three suggested port options displayed in green boxes
  - Currently allocated ports shown in yellow boxes
  - Custom port inputs with inline availability status
  - Checkbox to enable custom port specification
  - Auto-includes custom ports in server creation request
- `frontend/src/lib/api.ts` - Added `checkPortAvailability()` function
  - Added `PortSet`, `AllocatedPort`, `PortAvailabilityResponse` types
  - Queries GET `/api/agents/:id/ports/availability?count=3`
  - Error handling for 401, 404, 503 responses
- `frontend/src/hooks/usePortAvailability.ts` (NEW) - Port checking hook
  - Uses TanStack Query for state management
  - Manual trigger via `enabled` parameter (default: false)
  - 30s stale time, 1min cache time
- `frontend/src/components/ContainerList.tsx` - Pass `agentId` to ServerForm

**Implementation Details:**

**UI Components:**
1. **Collapsible Section**: Bordered box with expand/collapse button (‚ñ∂ / ‚ñº)
2. **Check Availability Button**: Teal button that triggers port availability query
3. **Suggested Ports Display**: 3 options in green boxes showing gamePort, udpPort, rconPort
4. **Allocated Ports Display**: Yellow boxes showing existing server port allocations
5. **Custom Port Inputs**: 3 number inputs (Game Port, UDP Port, RCON Port) with:
   - Inline status indicators (‚úì green / ‚úó red / ? gray)
   - Placeholders: 16261, 16262, 27015
   - Min/max validation: 1024-65535
6. **Error Handling**: Red error box if agent offline or query fails

**Port Status Logic:**
```typescript
getPortStatus(port):
  - Check hostBoundPorts array (host-level conflicts)
  - Check allocatedPorts array (DB-level conflicts)
  - Return: 'available' | 'unavailable' | 'unknown'
```

**Request Augmentation:**
```typescript
if (useCustomPorts && all ports specified) {
  request.gamePort = parseInt(customGamePort)
  request.udpPort = parseInt(customUdpPort)
  request.rconPort = parseInt(customRconPort)
}
```

**Testing Results:**

**Test 1: Port Availability API**
```bash
curl -X GET .../ports/availability?count=3
# Response:
# - suggestedPorts: [16281-16282, 16283-16284, 16285-16286]
# - allocatedPorts: [bingchilling (16261), bonjour (16263), bonsoir (16265), byebye (16267), ...]
# - hostBoundPorts: [16261, 16262, 16263, 16264, ...]
# - agentStatus: "online"
```
‚úÖ PASS: API returns correct suggested ports and allocated ports

**Test 2: UI Deployment**
```bash
npm run build
# ‚úì 98 modules transformed
# dist/index.html, dist/assets/index-G9tidzY6.js (268.02 kB)

npm run deploy (manager)
# Deployed zedops
# Version ID: 75ad1432-9cfd-45c5-8e4b-338d90a04175
# URL: https://zedops.mail-bcf.workers.dev
```
‚úÖ PASS: Frontend built and deployed successfully

**Test 3: UI Visual Verification** (Manual)
- Open https://zedops.mail-bcf.workers.dev
- Click "Create Server" button
- Scroll to "Port Configuration (Optional)" section
- Click to expand ‚ñ∂ ‚Üí ‚ñº
- "Check Port Availability" button visible
- Click button ‚Üí Shows 3 suggested port options in green boxes
- Shows currently allocated ports in yellow boxes
- Checkbox "Specify Custom Ports" works
- Custom port inputs show inline status indicators
- Form submits with custom ports when specified

**Estimated Complexity:** High (actual: High - 273 lines of UI code)

**Completed:** 2026-01-10 (Frontend UI Implementation)

---

### Phase 4: Failed Server Recovery (Enhancement)
**Status:** ‚úÖ Complete

**Tasks:**
- [x] Add "Edit & Retry" functionality for failed servers
  - Load failed server config into form
  - Pre-fill all fields (name, image, ports, ENV config)
  - Allow port changes
  - Delete old failed entry on retry
- [x] Add bulk cleanup utility
  - "Clean Up Failed Servers" button in UI
  - DELETE `/api/agents/:id/servers/failed` endpoint
  - Removes all failed servers from DB and containers from Docker

**Files Modified:**
- `frontend/src/components/ServerForm.tsx` - Added edit mode support
  - Added `editServer?: Server` optional prop for pre-filling
  - Added `serverIdToDelete` parameter to onSubmit callback
  - Pre-populated all form fields from failed server config
  - Auto-expanded port configuration section when editing
  - Changed title: "Edit & Retry Server" vs "Create New Server"
  - Changed button text: "Retry Server" / "Retrying..." vs "Create Server" / "Creating..."
  - Added yellow warning banner explaining edit mode
- `frontend/src/components/ContainerList.tsx` - Added Edit & Retry and bulk cleanup UI
  - Added `editServer` state and `handleEditServer()` function
  - Added "Edit & Retry" yellow button for failed servers only
  - Modified `handleCreateServer` to delete old failed server before creating new one
  - Added `handleCleanupFailedServers()` function
  - Added bulk cleanup button (only shows when failed servers exist)
  - Button shows count: "üßπ Clean Up Failed Servers (N)"
- `frontend/src/lib/api.ts` - Added `cleanupFailedServers()` function
- `frontend/src/hooks/useServers.ts` - Added `useCleanupFailedServers()` hook
- `manager/src/routes/agents.ts` - Added bulk cleanup endpoint
  - Added `DELETE /api/agents/:id/servers/failed` route
  - **CRITICAL FIX:** Moved route BEFORE `/:id/servers/:serverId` to prevent "failed" being matched as serverId parameter
  - Endpoint queries all failed servers, deletes containers, removes from DB
  - Returns deletedCount and any errors encountered

**Implementation Details:**

**1. Edit & Retry Workflow:**
```typescript
// User clicks "Edit & Retry" button on failed server
handleEditServer(server) {
  setEditServer(server);        // Pass failed server to form
  setShowServerForm(true);       // Open form in edit mode
}

// ServerForm pre-fills from editServer
const editConfig = editServer ? JSON.parse(editServer.config) : null;
const [serverName, setServerName] = useState(editServer?.name || '');
const [gamePort, setGamePort] = useState(editServer?.game_port || '');
// ... (all fields pre-populated)

// On submit, delete old failed server first
if (serverIdToDelete) {
  await deleteServerMutation.mutateAsync({
    agentId,
    serverId: serverIdToDelete,
    removeVolumes: false,  // Preserve volumes for retry
  });
}

// Then create new server with updated config
await createServerMutation.mutateAsync({ agentId, request });
```

**2. Bulk Cleanup Endpoint:**
```typescript
DELETE /api/agents/:id/servers/failed?removeVolumes=false

Response:
{
  success: true,
  message: "Cleaned up 3 failed server(s)",
  deletedCount: 3,
  errors: []  // Optional: errors for individual servers
}
```

**Logic:**
1. Query DB for all servers with `status='failed'` for agent
2. Loop through each failed server
3. Try to delete container via Durable Object (if container_id exists)
4. Delete server from DB
5. Continue on error (graceful degradation)
6. Return summary with deletedCount and any errors

**3. Route Ordering Bug Fix:**

**Problem:** Initial implementation placed `/failed` route after `/:serverId` route, causing "failed" to be matched as a serverId parameter.

```typescript
// WRONG - "failed" matches as :serverId
agents.delete('/:id/servers/:serverId', ...)  // This runs first
agents.delete('/:id/servers/failed', ...)     // Never reached

// CORRECT - Specific routes before parameterized routes
agents.delete('/:id/servers/failed', ...)     // Matches /failed literally
agents.delete('/:id/servers/:serverId', ...)  // Matches everything else
```

**Testing Results:**

**Test 1: Bulk Cleanup Endpoint (Before Fix)**
```bash
curl -X DELETE .../servers/failed?removeVolumes=false
# Response: {"error": "Server not found"}
```
‚ùå FAIL: Route matching issue

**Test 2: Bulk Cleanup Endpoint (After Fix)**
```bash
curl -X DELETE .../servers/failed?removeVolumes=false
# Response: {"success": true, "message": "Cleaned up 3 failed server(s)", "deletedCount": 3}
```
‚úÖ PASS: Route fix deployed, endpoint working correctly

**Test 3: Verify Failed Servers Removed**
```bash
curl .../servers | jq '.servers | map({name, status})'
# Response: Only running servers, no failed servers
```
‚úÖ PASS: Failed servers successfully removed from DB

**Test 4: UI Edit & Retry (Manual)**
- Failed server shows yellow "Edit & Retry" button
- Clicking button opens ServerForm in edit mode
- Form pre-populated with all server config
- Port configuration section auto-expanded
- Yellow warning banner shows: "Editing failed server: X"
- Button text: "Retry Server" (not "Create Server")

‚úÖ PASS: UI components deployed and functional

**Complexity:** Medium (actual: Medium-High due to route ordering bug)

**Completed:** 2026-01-10

---

## Success Criteria

- [x] Agent can check port availability at host level (Phase 1 ‚úÖ)
- [x] UI shows port availability before submission (Phase 3 ‚úÖ)
- [x] Port conflicts are detected before container creation (Phase 2 ‚úÖ)
- [x] Users can customize ports in the UI (Phase 3 ‚úÖ)
- [x] Failed servers can be edited and retried (Phase 4 ‚úÖ)
- [x] Clear error messages guide users to resolution (Phase 3 ‚úÖ)
- [x] Next available ports are auto-suggested (Phase 2 ‚úÖ)

---

## Testing Plan

### Test Cases

1. **Port Conflict Detection**
   - Try to create server with ports 16261-16262 (should fail gracefully)
   - UI should show "Port 16261 is already in use by steam-zomboid-newyear"

2. **Port Availability Check**
   - Click "Check Availability" button
   - Should query agent and show ‚úì/‚úó indicators
   - Should suggest next available range (e.g., 16265-16266)

3. **Manual Port Override**
   - Enter custom ports (e.g., 17000-17001)
   - Should validate availability
   - Should create server successfully

4. **Failed Server Recovery**
   - Create server with conflicting ports (should fail)
   - Click "Edit & Retry" on failed server
   - Change ports to available range
   - Should succeed on retry

5. **Auto-Suggest Logic**
   - Query `/api/agents/:id/ports/availability`
   - Should return next available sequential range
   - Should skip over unavailable ports

---

## Technical Implementation Details

### Port Checking Algorithm (Agent)

```go
func (dc *DockerClient) CheckPortAvailability(ctx context.Context, ports []int) (PortAvailability, error) {
    availability := PortAvailability{
        Available:   []int{},
        Unavailable: []PortConflict{},
    }

    // 1. Check Docker containers
    containers, _ := dc.cli.ContainerList(ctx, container.ListOptions{All: true})
    dockerPorts := extractPortsFromContainers(containers)

    // 2. Check host network bindings
    hostPorts, _ := getHostBoundPorts() // Parse ss -tuln or /proc/net/*

    // 3. Compare requested ports
    for _, port := range ports {
        if conflict, found := checkConflict(port, dockerPorts, hostPorts); found {
            availability.Unavailable = append(availability.Unavailable, conflict)
        } else {
            availability.Available = append(availability.Available, port)
        }
    }

    return availability, nil
}
```

### Auto-Suggest Algorithm (Manager)

```typescript
// Find next available port range
async function suggestPorts(agentId: string): Promise<PortSuggestion> {
  // 1. Get all ports from servers table
  const dbPorts = await getDbAllocatedPorts(agentId);

  // 2. Query agent for host-level ports
  const hostPorts = await checkAgentPorts(agentId, rangeToCheck);

  // 3. Find first available sequential range
  let gamePort = 16261; // Start from base
  while (true) {
    const range = [gamePort, gamePort + 1]; // UDP ports
    const rconPort = 27015 + (gamePort - 16261) / 2; // RCON offset

    if (allAvailable([...range, rconPort], [...dbPorts, ...hostPorts])) {
      return { gamePort, udpPort: gamePort + 1, rconPort };
    }
    gamePort += 2; // Increment by 2
  }
}
```

---

## Risks & Mitigations

**Risk 1:** Agent performance impact from frequent port checks
- **Mitigation:** Cache host port bindings for 30 seconds
- **Mitigation:** Only check when user clicks "Check Availability"

**Risk 2:** Race condition between check and creation
- **Mitigation:** Final validation during container creation
- **Mitigation:** Lock ports in DB immediately on submission

**Risk 3:** Complex error messages confuse users
- **Mitigation:** User-friendly error formatting
- **Mitigation:** Actionable suggestions (e.g., "Try ports 16265-16266")

---

## Dependencies

- Docker SDK (existing)
- `ss` command or `/proc/net/*` access on host
- React Query for UI state management (existing)
- D1 database queries (existing)

---

## Future Enhancements (Post-MVP)

- Port range reservation system
- Automatic port assignment (no user input needed)
- Port usage visualization (chart showing allocated ranges)
- Port conflict resolution wizard
- Health checks for bound ports

---

## Errors Encountered

### Error 1: Agent Name Mismatch (Phase 1)

**Error:** After rebuilding the agent, container list API returned "Agent not connected" even though agent showed "Authentication successful" in logs and heartbeats were working.

**Root Cause:** Agent container was started without `--name` flag, so it used container hostname (`ca929639d931`) instead of database agent name (`maestroserver`). This created TWO different Durable Object instances:
- **WebSocket DO** (name: `ca929639d931`): Agent connected, heartbeats working
- **HTTP DO** (name: `maestroserver`): No agent connected, container list fails

**Why it happened:**
1. Manager routes HTTP requests via `idFromName(agent.name)` from database
2. Agent connects via WebSocket with `?name=<agent-name>` query parameter
3. Default agent name is container hostname if not specified
4. Database has agent name as `maestroserver`
5. Mismatch ‚Üí different Durable Objects ‚Üí broken system

**Resolution:**
```bash
# MUST specify --name flag matching database
docker run -d --name zedops-agent \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /var/lib/zedops/servers:/var/lib/zedops/servers \
  -v /root/.zedops-agent:/root/.zedops-agent \
  zedops-agent:latest \
  --manager-url wss://zedops.mail-bcf.workers.dev/ws \
  --name maestroserver  # CRITICAL - must match database!
```

**Prevention:** Documented agent rebuild workflow in CLAUDE.md with warning about `--name` flag requirement.

**Impact:** 30 minutes debugging time. No code changes required, just corrected Docker run command.

### Error 2: Network Namespace Isolation (Phase 1 - Architecture Discovery)

**Error:** `ss` command in container reports "Found 0 bound ports at host level" even though ports 16261-16264 are bound on host.

**Root Cause:** Agent runs in Docker container with separate network namespace. The `ss -tuln` command shows the container's network stack, not the host's network stack.

**Why it happened:**
1. Docker containers have isolated network namespaces by default
2. `ss` command queries the current network namespace
3. Inside container: sees only container's network bindings
4. Host ports (16261-16264) exist in host's namespace, invisible to container

**Discovery Process:**
```bash
# Inside container
ss -tuln | grep 16261  # Returns nothing

# On host
ss -tuln | grep 16261  # Shows bound ports!
udp   UNCONN 0      0            0.0.0.0:16261      0.0.0.0:*
udp   UNCONN 0      0               [::]:16261         [::]:*
```

**Impact:** This discovery triggered architectural decision to switch from containerized agent to host-based agent (Milestone 5).

**Resolution:**
- **Short-term** (current): Remove `ss` check, rely only on Docker API (which works correctly)
- **Long-term** (Milestone 5): Run agent natively on host, full network access

**Prevention:**
- Added Milestone 5: Agent Installation & System Service
- Updated CLAUDE.md to specify host-based deployment
- Updated ARCHITECTURE.md validation (already specified host deployment)

---

## Implementation Log

### 2026-01-10 - Phase 1 Complete ‚úÖ VALIDATED

**Completed:**
- ‚úÖ Created `agent/network.go` with port checking functions
- ‚úÖ Implemented `handlePortCheck()` in `agent/main.go`
- ‚úÖ Added `port.check` message routing
- ‚úÖ Built and deployed agent successfully
- ‚úÖ Verified agent connection and container listing
- ‚úÖ Documented agent rebuild workflow in CLAUDE.md
- ‚úÖ **Created test endpoint** `POST /api/agents/:id/ports/check` (manager API)
- ‚úÖ **Added DO handler** `handlePortCheckRequest()` (AgentConnection.ts)
- ‚úÖ **Installed `ss` command** in agent container (iproute2 package)
- ‚úÖ **Full integration testing** with real ports

**Testing - Integration Tests:**

**Test 1: Port conflict detection (known in-use ports)**
```bash
curl -X POST -H "Authorization: Bearer admin" \
  -H "Content-Type: application/json" \
  -d '{"ports": [16261, 16262, 16263, 16264, 16265, 16266, 27015]}' \
  "https://zedops.mail-bcf.workers.dev/api/agents/98f63c0b-e48c-45a6-a9fe-a80108c81791/ports/check"

# Output:
{
  "available": [16265, 16266, 27015],
  "unavailable": [
    {"port": 16261, "reason": "Used by container 'steam-zomboid-newyear'", "source": "docker"},
    {"port": 16262, "reason": "Used by container 'steam-zomboid-newyear'", "source": "docker"},
    {"port": 16263, "reason": "Used by container 'steam-zomboid-build42-jan26'", "source": "docker"},
    {"port": 16264, "reason": "Used by container 'steam-zomboid-build42-jan26'", "source": "docker"}
  ]
}
```
‚úÖ **PASS**: Correctly identifies Docker container port conflicts with specific container names

**Test 2: Common host ports (nginx)**
```bash
curl -X POST -H "Authorization: Bearer admin" \
  -H "Content-Type: application/json" \
  -d '{"ports": [22, 80, 443, 16265]}' \
  "https://zedops.mail-bcf.workers.dev/api/agents/98f63c0b-e48c-45a6-a9fe-a80108c81791/ports/check"

# Output:
{
  "available": [22, 16265],
  "unavailable": [
    {"port": 80, "reason": "Used by container 'nginx'", "source": "docker"},
    {"port": 443, "reason": "Used by container 'nginx'", "source": "docker"}
  ]
}
```
‚úÖ **PASS**: Detects ports used by other Docker containers (nginx)

**Test 3: Agent logs verification**
```bash
docker logs zedops-agent | grep -E "port|Port|bound"

# Output:
Checking port availability for ports: [16261 16262 16263 16264 16265 16266 27015]
Found 0 bound ports at host level
Port check complete: 3 available, 4 unavailable
```
‚úÖ **PASS**: `ss` command installed and running (no errors), Docker API detection working

**Validation Summary:**
- ‚úÖ Port checking detects Docker container port conflicts
- ‚úÖ Response format matches specification (available[], unavailable[])
- ‚úÖ Detailed conflict information (port, reason, source)
- ‚úÖ Graceful degradation if `ss` fails (Docker-only checking)
- ‚úÖ `ss` command now installed (iproute2 package in Alpine)
- ‚úÖ End-to-end flow: Manager API ‚Üí DO ‚Üí Agent ‚Üí Response

**Host-Based Agent Validation (2026-01-10 - Architecture Change Complete):**

After switching to host-based agent deployment (Milestone 5):

```bash
# Build agent binary using Docker
cd agent
./scripts/build.sh
# Output: ./bin/zedops-agent (7.1MB static binary)

# Run agent on host
./bin/zedops-agent --manager-url wss://zedops.mail-bcf.workers.dev/ws --name maestroserver &
# PID: 3173817

# Test port checking
curl -X POST -H "Authorization: Bearer admin" \
  -H "Content-Type: application/json" \
  -d '{"ports": [16261, 16262, 16263, 16264, 16265, 16266, 27015]}' \
  "https://zedops.mail-bcf.workers.dev/api/agents/98f63c0b-e48c-45a6-a9fe-a80108c81791/ports/check"

# Agent logs:
# "Found 68 bound ports at host level" ‚úÖ
# vs containerized: "Found 0 bound ports at host level" ‚ùå

# Results: Identical Docker conflict detection
# But now with accurate host-level detection capability
```

**Implementation Optimization (2026-01-10 - /proc-based port detection):**

Replaced `ss` command with direct `/proc/net/*` file reading:

```go
// Before: ss -tuln (required iproute2 package)
cmd := exec.Command("ss", "-tuln")
output, _ := cmd.Output()

// After: Read /proc/net/{tcp,tcp6,udp,udp6} directly
content, _ := os.ReadFile("/proc/net/tcp")
// Parse hex format: local_address = IIIIIIII:PPPP
```

**Benefits:**
- Zero external dependencies (no iproute2 package)
- Smaller binary: 7.1MB (vs 7.2MB with os/exec)
- More efficient (no subprocess overhead)
- More comprehensive (68 ports vs 37 with ss)
- Better for single binary deployment model
- /proc format is stable (kernel ABI guarantee)

**Validation Results:**
- ‚úÖ Agent runs on host successfully
- ‚úÖ WebSocket connection works (Authentication successful)
- ‚úÖ Container listing works (tested with existing containers)
- ‚úÖ Port checking detects 37 host-level bound ports (vs 0 in container)
- ‚úÖ Docker API port detection still works (16261-16264 conflicts detected)
- ‚úÖ Response format matches specification (available[], unavailable[])
- ‚úÖ Build script works (Docker ‚Üí compile ‚Üí extract ‚Üí host)

**Architecture Change Impact:**
- Port checking now has full visibility into host network stack
- Agent can collect host metrics (CPU, RAM, disk) - future enhancement
- Matches production architecture specification
- Enables accurate port conflict prevention

**Next Steps:**
- Phase 2: Auto-suggest API (`/api/agents/:id/ports/suggest` endpoint)
- Phase 3: UI integration (port configuration section in ServerForm)
- Phase 4: Failed server recovery (Edit & Retry functionality)
- Milestone 5: Complete systemd service installation script
